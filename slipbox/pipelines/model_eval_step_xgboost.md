# XGBoost Model Evaluation Step

## Task Summary
The XGBoost Model Evaluation Step evaluates a trained XGBoost model on a specified dataset. This step:

1. Takes a trained XGBoost model and evaluation dataset as inputs
2. Generates predictions using the model on the evaluation data
3. Calculates various evaluation metrics (AUC, average precision, F1 score, etc.)
4. Outputs both the predictions and the calculated metrics to S3

## Input and Output Format

### Input
- **Model Input**: Trained XGBoost model artifacts from a previous training step
- **Evaluation Data**: Dataset to evaluate the model on (training, validation, testing, or calibration)
- **Optional Dependencies**: List of pipeline steps that must complete before this step runs

### Output
- **Evaluation Output**: Predictions generated by the model on the evaluation data
- **Metrics Output**: Calculated evaluation metrics (AUC, precision, recall, etc.)
- **ProcessingStep**: A configured SageMaker pipeline step that can be added to a pipeline

## Configuration Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| processing_entry_point | Entry point script for model evaluation | model_evaluation_xgboost.py |
| processing_source_dir | Directory containing processing scripts | Required |
| processing_instance_type_small | Instance type for small processing | Inherited from base |
| processing_instance_type_large | Instance type for large processing | Inherited from base |
| processing_instance_count | Number of instances for processing | Inherited from base |
| processing_volume_size | EBS volume size for processing | Inherited from base |
| use_large_processing_instance | Whether to use large instance type | Inherited from base |
| job_type | Which split to evaluate on | calibration |
| hyperparameters | Model hyperparameters config | Required |
| eval_metric_choices | List of evaluation metrics to compute | ["auc", "average_precision", "f1_score"] |
| xgboost_framework_version | XGBoost framework version for processing | 1.5-1 |

## Validation Rules
- processing_entry_point must be provided
- job_type must be one of: 'training', 'calibration', 'validation', 'test'
- hyperparameters must be an instance of ModelHyperparameters
- Required input names 'model_input' and 'eval_data_input' must be defined
- Required output names 'eval_output' and 'metrics_output' must be defined

## Usage Example
```python
from src.pipelines.config_model_eval_step_xgboost import XGBoostModelEvalConfig
from src.pipelines.builder_model_eval_step_xgboost import XGBoostModelEvalStepBuilder
from src.pipelines.hyperparameters_xgboost import XGBoostHyperparameters

# Create hyperparameters
hyperparams = XGBoostHyperparameters(
    id_name="customer_id",
    label_name="target",
    feature_names=["feature1", "feature2", "feature3"]
)

# Create configuration
config = XGBoostModelEvalConfig(
    processing_entry_point="model_evaluation_xgboost.py",
    processing_source_dir="s3://my-bucket/scripts/",
    job_type="validation",
    hyperparameters=hyperparams,
    eval_metric_choices=["auc", "precision", "recall", "f1_score"]
)

# Create builder
builder = XGBoostModelEvalStepBuilder(config=config)

# Define input and output locations
inputs = {
    "model_input": training_step.properties.ModelArtifacts.S3ModelArtifacts,
    "eval_data_input": "s3://my-bucket/validation-data/"
}

outputs = {
    "eval_output": "s3://my-bucket/evaluation-results/predictions/",
    "metrics_output": "s3://my-bucket/evaluation-results/metrics/"
}

# Create step
eval_step = builder.create_step(
    inputs=inputs,
    outputs=outputs,
    dependencies=[training_step]
)

# Add to pipeline
pipeline.add_step(eval_step)
```

## Environment Variables
The evaluation step sets the following environment variables for the processing job:
- **ID_FIELD**: The name of the ID field from hyperparameters
- **LABEL_FIELD**: The name of the label field from hyperparameters

## Input and Output Channels
### Input Channels
- **model_input**: Model artifacts input (destination: /opt/ml/processing/input/model)
- **eval_data_input**: Evaluation data input (destination: /opt/ml/processing/input/eval_data)

### Output Channels
- **eval_output**: Output for evaluation predictions (source: /opt/ml/processing/output/eval)
- **metrics_output**: Output for evaluation metrics (source: /opt/ml/processing/output/metrics)
