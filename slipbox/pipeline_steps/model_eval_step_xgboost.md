# XGBoost Model Evaluation Step

## Task Summary
The XGBoost Model Evaluation Step evaluates a trained XGBoost model on a specified dataset. This step:

1. Takes a trained XGBoost model and evaluation dataset as inputs
2. Generates predictions using the model on the evaluation data
3. Calculates various evaluation metrics (AUC, average precision, F1 score, etc.)
4. Outputs both the predictions and the calculated metrics to S3

## Input and Output Format

### Input
- **Model Input**: Trained XGBoost model artifacts from a previous training step
- **Evaluation Data**: Dataset to evaluate the model on (training, validation, testing, or calibration)
- **Optional Dependencies**: List of pipeline steps that must complete before this step runs

### Output
- **Evaluation Output**: Predictions generated by the model on the evaluation data
- **Metrics Output**: Calculated evaluation metrics (AUC, precision, recall, etc.)
- **ProcessingStep**: A configured SageMaker pipeline step that can be added to a pipeline

## Configuration Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| processing_entry_point | Entry point script for model evaluation | model_evaluation_xgboost.py |
| processing_source_dir | Directory containing processing scripts | Required |
| processing_instance_type_small | Instance type for small processing | Inherited from base |
| processing_instance_type_large | Instance type for large processing | Inherited from base |
| processing_instance_count | Number of instances for processing | Inherited from base |
| processing_volume_size | EBS volume size for processing | Inherited from base |
| use_large_processing_instance | Whether to use large instance type | Inherited from base |
| job_type | Which split to evaluate on | calibration |
| hyperparameters | Model hyperparameters config | Required |
| eval_metric_choices | List of evaluation metrics to compute | ["auc", "average_precision", "f1_score"] |
| xgboost_framework_version | XGBoost framework version for processing | 1.5-1 |

## Validation Rules
- processing_entry_point must be provided
- job_type must be one of: 'training', 'calibration', 'validation', 'test'
- hyperparameters must be an instance of ModelHyperparameters
- Required input names 'model_input' and 'eval_data_input' must be defined
- Required output names 'eval_output' and 'metrics_output' must be defined

## Usage Example
```python
from src.pipeline_steps.config_model_eval_step_xgboost import XGBoostModelEvalConfig
from src.pipeline_steps.builder_model_eval_step_xgboost import XGBoostModelEvalStepBuilder

# Create configuration
config = XGBoostModelEvalConfig(
    processing_entry_point="model_evaluation_xgb.py",
    processing_source_dir="s3://my-bucket/scripts/",
    job_type="calibration",
    input_names={
        "model_input": "ModelArtifacts",
        "data_input": "CalibrationData",
        "metadata_input": "CalibrationMetadata"
    },
    output_names={
        "metrics_output": "EvaluationMetrics",
        "plots_output": "EvaluationPlots"
    }
)

# Create builder
builder = XGBoostModelEvalStepBuilder(config=config)

# Define input and output locations
inputs = {
    "model_input": "s3://my-bucket/model-artifacts/",
    "data_input": "s3://my-bucket/calibration-data/",
    "metadata_input": "s3://my-bucket/calibration-metadata/"
}

outputs = {
    "metrics_output": "s3://my-bucket/evaluation-metrics/",
    "plots_output": "s3://my-bucket/evaluation-plots/"
}

# Create step
eval_step = builder.create_step(
    inputs=inputs,
    outputs=outputs,
    dependencies=[training_step, calibration_data_step]
)

# Add to pipeline
pipeline.add_step(eval_step)
```

## Integration with Pipeline Builder Template

### Input Arguments

The `XGBoostModelEvalStepBuilder` defines the following input arguments that can be automatically connected by the Pipeline Builder Template:

| Argument | Description | Required | Source |
|----------|-------------|----------|--------|
| model_input | Model artifacts location | Yes | Previous step's model_artifacts output |
| data_input | Calibration data location | Yes | Previous step's processed_data output |
| metadata_input | Metadata input location | No | Previous step's metadata output |

### Output Properties

The `XGBoostModelEvalStepBuilder` provides the following output properties that can be used by subsequent steps:

| Property | Description | Access Pattern |
|----------|-------------|---------------|
| metrics_output | Evaluation metrics location | `step.properties.ProcessingOutputConfig.Outputs["metrics_output"].S3Output.S3Uri` |
| plots_output | Evaluation plots location | `step.properties.ProcessingOutputConfig.Outputs["plots_output"].S3Output.S3Uri` |

### Usage with Pipeline Builder Template

When using the Pipeline Builder Template, the inputs and outputs are automatically connected based on the DAG structure:

```python
# Create the DAG
dag = PipelineDAG()
dag.add_node("data_load")
dag.add_node("preprocess")
dag.add_node("train")
dag.add_node("eval")
dag.add_edge("data_load", "preprocess")
dag.add_edge("preprocess", "train")
dag.add_edge("train", "eval")
dag.add_edge("preprocess", "eval")  # For calibration data

# Create the config map
config_map = {
    "data_load": data_load_config,
    "preprocess": preprocess_config,
    "train": train_config,
    "eval": eval_config,
}

# Create the step builder map
step_builder_map = {
    "CradleDataLoadStep": CradleDataLoadingStepBuilder,
    "TabularPreprocessingStep": TabularPreprocessingStepBuilder,
    "XGBoostTrainingStep": XGBoostTrainingStepBuilder,
    "XGBoostModelEvalStep": XGBoostModelEvalStepBuilder,
}

# Create the template
template = PipelineBuilderTemplate(
    dag=dag,
    config_map=config_map,
    step_builder_map=step_builder_map,
    sagemaker_session=sagemaker_session,
    role=role,
)

# Generate the pipeline
pipeline = template.generate_pipeline("my-pipeline")
```

For more details on how the Pipeline Builder Template handles connections between steps, see the [Pipeline Builder documentation](../pipeline_builder/README.md).

## Environment Variables
The evaluation step sets the following environment variables for the processing job:
- **ID_FIELD**: The name of the ID field from hyperparameters
- **LABEL_FIELD**: The name of the label field from hyperparameters

## Input and Output Channels
### Input Channels
- **model_input**: Model artifacts input (destination: /opt/ml/processing/input/model)
- **eval_data_input**: Evaluation data input (destination: /opt/ml/processing/input/eval_data)

### Output Channels
- **eval_output**: Output for evaluation predictions (source: /opt/ml/processing/output/eval)
- **metrics_output**: Output for evaluation metrics (source: /opt/ml/processing/output/metrics)
