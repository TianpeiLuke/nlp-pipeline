# XGBoost Model Evaluation Step

## Task Summary
The XGBoost Model Evaluation Step evaluates a trained XGBoost model on a specified dataset. This step:

1. Takes a trained XGBoost model and evaluation dataset as inputs
2. Generates predictions using the model on the evaluation data
3. Calculates various evaluation metrics (AUC, average precision, F1 score, etc.)
4. Outputs both the predictions and the calculated metrics to S3

The step now uses step specifications and script contracts to standardize input/output paths and dependencies.

## Input and Output Format

### Input
- **Model Input**: Trained XGBoost model artifacts from a previous training step
- **Evaluation Data**: Dataset to evaluate the model on (training, validation, testing, or calibration)
- **Optional Dependencies**: List of pipeline steps that must complete before this step runs

Note: The step can automatically extract inputs from dependencies using the dependency resolver.

### Output
- **Evaluation Output**: Predictions generated by the model on the evaluation data
- **Metrics Output**: Calculated evaluation metrics (AUC, precision, recall, etc.)
- **ProcessingStep**: A configured SageMaker pipeline step that can be added to a pipeline

## Configuration Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| processing_entry_point | Entry point script for model evaluation | model_evaluation_xgboost.py |
| processing_source_dir | Directory containing processing scripts | Required |
| processing_instance_type_small | Instance type for small processing | Inherited from base |
| processing_instance_type_large | Instance type for large processing | Inherited from base |
| processing_instance_count | Number of instances for processing | Inherited from base |
| processing_volume_size | EBS volume size for processing | Inherited from base |
| use_large_processing_instance | Whether to use large instance type | Inherited from base |
| job_type | Which split to evaluate on | calibration |
| hyperparameters | Model hyperparameters config | Required |
| eval_metric_choices | List of evaluation metrics to compute | ["auc", "average_precision", "f1_score"] |
| xgboost_framework_version | XGBoost framework version for processing | 1.5-1 |

## Validation Rules
- processing_entry_point must be provided
- job_type must be one of: 'training', 'calibration', 'validation', 'test'
- hyperparameters must be an instance of ModelHyperparameters
- Required input names 'model_input' and 'eval_data_input' must be defined
- Required output names 'eval_output' and 'metrics_output' must be defined

## Environment Variables
The evaluation step sets the following environment variables for the processing job:
- **ID_FIELD**: The name of the ID field from hyperparameters
- **LABEL_FIELD**: The name of the label field from hyperparameters

## Specification and Contract Support

The XGBoost Model Evaluation Step now uses:
- **Step Specification**: Defines input/output relationships and dependencies
- **Script Contract**: Defines expected container paths for script inputs/outputs

These help standardize integration with the Pipeline Builder Template and ensure consistent handling of inputs and outputs.

## Usage Example
```python
from src.pipeline_steps.config_model_eval_step_xgboost import XGBoostModelEvalConfig
from src.pipeline_steps.builder_model_eval_step_xgboost import XGBoostModelEvalStepBuilder
from src.pipeline_steps.hyperparameters_xgboost import XGBoostModelHyperparameters

# Create hyperparameters
hyperparams = XGBoostModelHyperparameters(
    id_name="customer_id",
    label_name="target"
    # other hyperparameter settings
)

# Create configuration
config = XGBoostModelEvalConfig(
    processing_entry_point="model_evaluation_xgb.py",
    processing_source_dir="s3://my-bucket/scripts/",
    job_type="calibration",
    hyperparameters=hyperparams,
    xgboost_framework_version="1.5-1",
    pipeline_name="xgboost-evaluation"
)

# Create builder and step
builder = XGBoostModelEvalStepBuilder(config=config)
eval_step = builder.create_step(
    # The step can extract inputs from dependencies automatically
    dependencies=[training_step, preprocessing_step]
)

# Add to pipeline
pipeline.add_step(eval_step)
```

## Input and Output Channels
### Input Channels
- **model_input**: Model artifacts input (destination: /opt/ml/processing/input/model)
- **eval_data_input**: Evaluation data input (destination: /opt/ml/processing/input/eval_data)

### Output Channels
- **eval_output**: Output for evaluation predictions (source: /opt/ml/processing/output/eval)
- **metrics_output**: Output for evaluation metrics (source: /opt/ml/processing/output/metrics)

## Integration with Pipeline Builder Template

### Input Arguments

The `XGBoostModelEvalStepBuilder` defines the following input arguments that can be automatically connected by the Pipeline Builder Template:

| Argument | Description | Required | Source |
|----------|-------------|----------|--------|
| model_input | Model artifacts location | Yes | Previous step's model_artifacts output |
| eval_data_input | Evaluation data location | Yes | Previous step's processed_data output |

### Output Properties

The `XGBoostModelEvalStepBuilder` provides the following output properties that can be used by subsequent steps:

| Property | Description | Access Pattern |
|----------|-------------|---------------|
| metrics_output | Evaluation metrics location | `step.properties.ProcessingOutputConfig.Outputs["metrics_output"].S3Output.S3Uri` |
| eval_output | Evaluation predictions location | `step.properties.ProcessingOutputConfig.Outputs["eval_output"].S3Output.S3Uri` |

### Usage with Pipeline Builder Template

When using the Pipeline Builder Template, the inputs and outputs are automatically connected based on the DAG structure:

```python
# Create the DAG
dag = PipelineDAG()
dag.add_node("data_load")
dag.add_node("preprocess")
dag.add_node("train")
dag.add_node("eval")
dag.add_edge("data_load", "preprocess")
dag.add_edge("preprocess", "train")
dag.add_edge("train", "eval")
dag.add_edge("preprocess", "eval")  # For calibration data

# Create the config map
config_map = {
    "data_load": data_load_config,
    "preprocess": preprocess_config,
    "train": train_config,
    "eval": eval_config,
}

# Create the step builder map
step_builder_map = {
    "CradleDataLoadStep": CradleDataLoadingStepBuilder,
    "TabularPreprocessingStep": TabularPreprocessingStepBuilder,
    "XGBoostTrainingStep": XGBoostTrainingStepBuilder,
    "XGBoostModelEvalStep": XGBoostModelEvalStepBuilder,
}

# Create the template
template = PipelineBuilderTemplate(
    dag=dag,
    config_map=config_map,
    step_builder_map=step_builder_map,
    sagemaker_session=sagemaker_session,
    role=role,
)

# Generate the pipeline
pipeline = template.generate_pipeline("my-pipeline")
```

For more details on how the Pipeline Builder Template handles connections between steps, see the [Pipeline Builder documentation](../pipeline_builder/README.md).
