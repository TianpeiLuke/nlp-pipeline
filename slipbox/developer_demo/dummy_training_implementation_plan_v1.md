# Implementation Plan for DummyTraining Step

## 1. Step Overview
- **Purpose**: Create a training step that bypasses actual training by taking a pretrained model.tar.gz file from a local folder and making it available for downstream packaging and registration steps
- **Inputs**: Path to a pretrained model.tar.gz file
- **Outputs**: S3 path to the uploaded model artifacts
- **Position in pipeline**: Between data preprocessing and model packaging/registration steps
- **Architectural considerations**: 
  - Using a Processing step instead of a Training step since we're not actually training, just moving a file
  - Must maintain compatibility with existing step interfaces for seamless integration
- **Alignment with design principles**: Follows the specification-driven architecture with clear separation of concerns across the four layers

## 2. Components to Create

### Script Contract: src/pipeline_script_contracts/dummy_training_contract.py
- **Input paths**:
  - `pretrained_model_path`: "/opt/ml/processing/input/model/model.tar.gz" (Path to pretrained model file)
- **Output paths**:
  - `model_output`: "/opt/ml/processing/output/model" (Where the model will be copied to)
- **Environment variables**: None required

### Step Specification: src/pipeline_step_specs/dummy_training_spec.py
- **Dependencies**:
  - `pretrained_model_path`: Path to pretrained model file (DependencyType.PROCESSING_INPUT)
    - Compatible sources: Any step that can provide a model file path
- **Outputs**:
  - `model_output`: S3 path to the model artifacts (DependencyType.PROCESSING_OUTPUT)
    - Property path: "properties.ProcessingOutputConfig.Outputs['model_output'].S3Output.S3Uri"

### Configuration: src/pipeline_steps/config_dummy_training.py
- **Step-specific parameters**:
  - `pretrained_model_path`: Local path to the pretrained model.tar.gz file
- **SageMaker parameters**:
  - instance_type: Small instance type, default "ml.m5.large"
  - instance_count: Default 1
  - volume_size_gb: Default 30
  - max_runtime_seconds: Default 1200 (20 minutes)

### Step Builder: src/pipeline_steps/builder_dummy_training.py
- **Special handling**:
  - Upload the local pretrained model to S3
  - Create a minimal processing script to copy the model from input to output
  - Set up the processing step to execute this script

### Processing Script: src/pipeline_scripts/dummy_training.py
- **Algorithm**: Simple file copy from input to output location
- **Main functions**:
  - `copy_model`: Copy the model.tar.gz from input to output directory
  - `main`: Parse arguments, validate paths, and execute the copy

## 3. Files to Update
- src/pipeline_registry/step_names.py
  - Add "DummyTraining" to STEP_NAMES dictionary
- src/pipeline_steps/__init__.py
  - Import and expose DummyTrainingStepBuilder
- src/pipeline_step_specs/__init__.py
  - Import and expose DUMMY_TRAINING_SPEC
- src/pipeline_script_contracts/__init__.py
  - Import and expose DUMMY_TRAINING_CONTRACT

## 4. Integration Strategy
- **Upstream steps**: Any step that can provide a path to a model.tar.gz file
- **Downstream steps**: ModelPackagingStep, MIMSRegistrationStep
- **DAG updates**: Add DummyTraining node between preprocessing and packaging steps

## 5. Contract-Specification Alignment
- **Input alignment**: 
  - Contract input path `pretrained_model_path` maps to specification dependency `pretrained_model_path`
- **Output alignment**: 
  - Contract output path `model_output` maps to specification output `model_output`
- **Validation strategy**:
  - Use existing validation utilities to ensure input/output paths match between contract and specification

## 6. Error Handling Strategy
- **Input validation**: 
  - Verify the pretrained model.tar.gz file exists
  - Validate file integrity before processing
- **Script robustness**: 
  - Handle missing input files gracefully
  - Provide meaningful error messages for common failure modes
- **Logging strategy**: 
  - Log file sizes and paths for debugging
  - Clearly indicate success or failure of the copy operation
- **Error reporting**: 
  - Raise specific exceptions with descriptive messages
  - Include traceback information for debugging

## 7. Testing and Validation Plan
- **Unit tests**:
  - Test DummyTrainingStepBuilder in isolation
  - Verify contract-specification alignment
- **Integration tests**:
  - Test end-to-end flow from preprocessing to packaging using the dummy training step
- **Validation criteria**:
  - Successful copy of model.tar.gz to the output location
  - Correct property paths in specification outputs
  - Compatible integration with downstream model packaging steps

## Implementation Details

### 1. Step Registry Addition

```python
# In src/pipeline_registry/step_names.py
STEP_NAMES = {
    # ... existing steps ...
    
    "DummyTraining": {
        "config_class": "DummyTrainingConfig",
        "builder_step_name": "DummyTrainingStep",
        "spec_type": "DummyTraining",
        "description": "Training step that uses a pretrained model"
    },
}
```

### 2. Script Contract Implementation

```python
# src/pipeline_script_contracts/dummy_training_contract.py
from .base_script_contract import ScriptContract

DUMMY_TRAINING_CONTRACT = ScriptContract(
    entry_point="dummy_training.py",
    expected_input_paths={
        "pretrained_model_path": "/opt/ml/processing/input/model/model.tar.gz"
    },
    expected_output_paths={
        "model_output": "/opt/ml/processing/output/model"
    },
    required_env_vars=[],
    optional_env_vars={},
    framework_requirements={
        "boto3": ">=1.26.0"
    },
    description="Contract for dummy training step that copies a pretrained model.tar.gz to output location"
)
```

### 3. Step Specification Implementation

```python
# src/pipeline_step_specs/dummy_training_spec.py
from ..pipeline_deps.base_specifications import StepSpecification, NodeType, DependencySpec, OutputSpec, DependencyType
from ..pipeline_registry.step_names import get_spec_step_type

def _get_dummy_training_contract():
    from ..pipeline_script_contracts.dummy_training_contract import DUMMY_TRAINING_CONTRACT
    return DUMMY_TRAINING_CONTRACT

DUMMY_TRAINING_SPEC = StepSpecification(
    step_type=get_spec_step_type("DummyTraining"),
    node_type=NodeType.INTERNAL,
    script_contract=_get_dummy_training_contract(),
    dependencies=[
        DependencySpec(
            logical_name="pretrained_model_path",
            dependency_type=DependencyType.PROCESSING_INPUT,
            required=True,
            compatible_sources=["ProcessingStep", "LocalFile"],
            semantic_keywords=["model", "pretrained", "artifact"],
            data_type="S3Uri",
            description="Path to pretrained model.tar.gz file"
        )
    ],
    outputs=[
        OutputSpec(
            logical_name="model_output",
            output_type=DependencyType.PROCESSING_OUTPUT,
            property_path="properties.ProcessingOutputConfig.Outputs['model_output'].S3Output.S3Uri",
            data_type="S3Uri",
            description="S3 path to model artifacts",
            aliases=["ModelOutputPath", "ModelArtifacts", "model_data", "output_path", "model_input"]
        )
    ]
)
```

### 4. Configuration Class Implementation

```python
# src/pipeline_steps/config_dummy_training.py
from .config_processing_step_base import ProcessingStepConfigBase

class DummyTrainingConfig(ProcessingStepConfigBase):
    """Configuration for DummyTraining."""
    
    def __init__(
        self,
        region: str,
        pipeline_s3_loc: str,
        instance_type: str = "ml.m5.large",
        instance_count: int = 1,
        volume_size_gb: int = 30,
        max_runtime_seconds: int = 1200,
        pretrained_model_path: str = "",
    ):
        """Initialize DummyTraining configuration.
        
        Args:
            region: AWS region
            pipeline_s3_loc: S3 location for pipeline artifacts
            instance_type: SageMaker instance type
            instance_count: Number of instances
            volume_size_gb: EBS volume size in GB
            max_runtime_seconds: Maximum runtime in seconds
            pretrained_model_path: Local path to pretrained model.tar.gz
        """
        super().__init__(region, pipeline_s3_loc)
        self.instance_type = instance_type
        self.instance_count = instance_count
        self.volume_size_gb = volume_size_gb
        self.max_runtime_seconds = max_runtime_seconds
        self.pretrained_model_path = pretrained_model_path
    
    def get_script_contract(self):
        """Return the script contract for this step."""
        from ..pipeline_script_contracts.dummy_training_contract import DUMMY_TRAINING_CONTRACT
        return DUMMY_TRAINING_CONTRACT
```

### 5. Processing Script Implementation

```python
# src/pipeline_scripts/dummy_training.py
#!/usr/bin/env python
"""
DummyTraining Processing Script

This script copies a pretrained model.tar.gz file from the input location
to the output location. It serves as a dummy training step that skips actual
training and simply passes a pretrained model to downstream steps.
"""

import argparse
import logging
import os
import shutil
import sys
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def copy_model(input_path: str, output_dir: str) -> str:
    """
    Copy the pretrained model.tar.gz from input to output location.
    
    Args:
        input_path: Path to the input model.tar.gz file
        output_dir: Directory to copy the model to
        
    Returns:
        Path to the copied model file
    """
    logger.info(f"Input model path: {input_path}")
    logger.info(f"Output directory: {output_dir}")
    
    input_path = Path(input_path)
    output_dir = Path(output_dir)
    
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Verify input file exists
    if not input_path.exists():
        raise FileNotFoundError(f"Pretrained model file not found: {input_path}")
    
    # Copy the file
    output_path = output_dir / "model.tar.gz"
    logger.info(f"Copying {input_path} to {output_path}")
    shutil.copy2(input_path, output_path)
    
    # Verify copy was successful
    if output_path.exists() and output_path.stat().st_size == input_path.stat().st_size:
        logger.info(f"Successfully copied model file ({output_path.stat().st_size} bytes)")
    else:
        raise RuntimeError("Model file copy failed or size mismatch")
    
    return str(output_path)

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Dummy Training Processing Script")
    parser.add_argument(
        "--pretrained-model-path",
        type=str,
        required=True,
        help="Path to the pretrained model.tar.gz file"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Directory to save the copied model"
    )
    return parser.parse_args()

def main():
    """Main entry point."""
    args = parse_args()
    
    try:
        copied_model_path = copy_model(args.pretrained_model_path, args.output_dir)
        logger.info(f"Model successfully copied to {copied_model_path}")
        return 0
    except Exception as e:
        logger.error(f"Error copying model: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

### 6. Step Builder Implementation

```python
# src/pipeline_steps/builder_dummy_training.py
import logging
import os
from pathlib import Path
from typing import Dict, Optional, Any, List

from sagemaker.processing import Processor, ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.workflow.functions import Join
from sagemaker.s3 import S3Uploader

from .config_dummy_training import DummyTrainingConfig
from .builder_step_base import StepBuilderBase
from .s3_utils import S3PathHandler
from ..pipeline_deps.registry_manager import RegistryManager
from ..pipeline_deps.dependency_resolver import UnifiedDependencyResolver
from ..pipeline_step_specs.dummy_training_spec import DUMMY_TRAINING_SPEC

logger = logging.getLogger(__name__)

class DummyTrainingStep(StepBuilderBase):
    """Builder for DummyTraining processing step."""
    
    def __init__(
        self, 
        config: DummyTrainingConfig,
        sagemaker_session=None, 
        role=None, 
        notebook_root=None,
        registry_manager=None,
        dependency_resolver=None
    ):
        """Initialize the DummyTraining step builder."""
        if not isinstance(config, DummyTrainingConfig):
            raise ValueError("DummyTrainingStep requires a DummyTrainingConfig instance.")
        
        super().__init__(
            config=config,
            spec=DUMMY_TRAINING_SPEC,
            sagemaker_session=sagemaker_session,
            role=role,
            notebook_root=notebook_root,
            registry_manager=registry_manager,
            dependency_resolver=dependency_resolver
        )
        self.config: DummyTrainingConfig = config
    
    def validate_configuration(self):
        """Validate the provided configuration."""
        self.log_info("Validating DummyTrainingConfig...")
        
        # Check for required local file
        if not self.config.pretrained_model_path:
            raise ValueError("pretrained_model_path is required in DummyTrainingConfig")
        
        # Check if file exists (if path is concrete and not a variable)
        if not hasattr(self.config.pretrained_model_path, 'expr'):
            model_path = Path(self.config.pretrained_model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"Pretrained model not found at {model_path}")
        
        self.log_info("DummyTrainingConfig validation succeeded.")
    
    def _upload_model_to_s3(self) -> str:
        """Upload the pretrained model to S3."""
        self.log_info(f"Uploading pretrained model from {self.config.pretrained_model_path}")
        
        # Construct target S3 URI
        target_s3_uri = f"{self.config.pipeline_s3_loc}/dummy_training/input/model.tar.gz"
        
        # Upload the file
        S3Uploader.upload(
            self.config.pretrained_model_path,
            target_s3_uri,
            sagemaker_session=self.session
        )
        
        self.log_info(f"Uploaded model to {target_s3_uri}")
        return target_s3_uri
    
    def _get_processor(self):
        """Get the processor for the step."""
        return ScriptProcessor(
            image_uri="137112412989.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",
            command=["python3"],
            instance_type=self.config.instance_type,
            instance_count=self.config.instance_count,
            volume_size_in_gb=self.config.volume_size_gb,
            max_runtime_in_seconds=self.config.max_runtime_seconds,
            role=self.role,
            sagemaker_session=self.session,
            base_job_name=self._sanitize_name_for_sagemaker(
                f"{self._get_step_name('DummyTraining')}"
            )
        )
    
    def _get_inputs(self, inputs: Dict[str, Any]) -> List[ProcessingInput]:
        """Get inputs for the processor using the specification and contract."""
        processing_inputs = []
        
        # Use either the uploaded model or one provided through dependencies
        model_s3_uri = inputs.get("pretrained_model_path")
        if not model_s3_uri:
            # Upload the local model file if no S3 path is provided
            model_s3_uri = self._upload_model_to_s3()
        
        # Add model input
        processing_inputs.append(
            ProcessingInput(
                source=model_s3_uri,
                destination="/opt/ml/processing/input/model/model.tar.gz",
                input_name="model"
            )
        )
        
        return processing_inputs
    
    def _get_outputs(self, outputs: Dict[str, Any]) -> List[ProcessingOutput]:
        """Get outputs for the processor using the specification and contract."""
        # Use the pipeline S3 location to construct output path
        default_output_path = f"{self.config.pipeline_s3_loc}/dummy_training/output"
        output_path = outputs.get("model_output", default_output_path)
        
        return [
            ProcessingOutput(
                output_name="model_output",
                source="/opt/ml/processing/output/model",
                destination=output_path
            )
        ]
    
    def create_step(self, **kwargs) -> ProcessingStep:
        """Create the processing step.
        
        Args:
            **kwargs: Additional keyword arguments for step creation.
                     Should include 'dependencies' list if step has dependencies.
        """
        # Extract inputs from dependencies using the resolver
        dependencies = kwargs.get('dependencies', [])
        inputs = {}
        if dependencies:
            inputs = self.extract_inputs_from_dependencies(dependencies)
        
        # Create processor
        processor = self._get_processor()
        
        # Get processor inputs and outputs
        processing_inputs = self._get_inputs(inputs)
        processing_outputs = self._get_outputs({})
        
        # Create the step
        step_name = kwargs.get('step_name', 'DummyTraining')
        
        # Generate script arguments
        script_args = [
            "--pretrained-model-path", "/opt/ml/processing/input/model/model.tar.gz",
            "--output-dir", "/opt/ml/processing/output/model"
        ]
        
        step = processor.run(
            code="src/pipeline_scripts/dummy_training.py",
            inputs=processing_inputs,
            outputs=processing_outputs,
            arguments=script_args,
            job_name=self._generate_job_name(step_name),
            wait=False
        )
        
        # Store specification in step for future reference
        setattr(step, '_spec', self.spec)
        
        return step
