# Pipeline DAG

## What is the Purpose of Pipeline DAG?

Pipeline DAG (Directed Acyclic Graph) serves as the **structural foundation** that manages pipeline topology, dependency relationships, and execution flow. It provides the mathematical and computational framework for representing, validating, and optimizing ML pipeline workflows.

## Core Purpose

Pipeline DAG provides the **structural foundation layer** that:

1. **Pipeline Topology Management** - Represent pipeline structure as a directed acyclic graph
2. **Dependency Relationship Modeling** - Model complex dependencies between pipeline steps
3. **Execution Flow Control** - Determine optimal execution order and parallelization
4. **Cycle Detection and Prevention** - Ensure pipeline validity through cycle detection
5. **Graph Analysis and Optimization** - Enable advanced pipeline analysis and optimization

## Key Features

### 1. Pipeline Topology Management

Pipeline DAG represents the pipeline structure as a mathematical graph:

```python
class PipelineDAG:
    def __init__(self):
        self.nodes = {}  # step_id -> StepNode
        self.edges = {}  # (source_id, target_id) -> EdgeMetadata
        self.adjacency_list = defaultdict(list)  # source_id -> [target_ids]
        self.reverse_adjacency = defaultdict(list)  # target_id -> [source_ids]
    
    def add_node(self, step_id: str, step_spec: StepSpecification) -> StepNode:
        """Add a step node to the DAG"""
        node = StepNode(step_id, step_spec)
        self.nodes[step_id] = node
        
        # Validate node type constraints
        self._validate_node_constraints(node)
        
        return node
    
    def add_edge(self, source_id: str, target_id: str, edge_type: EdgeType) -> None:
        """Add a dependency edge between steps"""
        if source_id not in self.nodes or target_id not in self.nodes:
            raise ValueError("Both source and target nodes must exist")
        
        # Check for cycles
        if self._would_create_cycle(source_id, target_id):
            raise CycleDetectedError(f"Adding edge {source_id} -> {target_id} would create a cycle")
        
        # Add edge
        edge = EdgeMetadata(source_id, target_id, edge_type)
        self.edges[(source_id, target_id)] = edge
        self.adjacency_list[source_id].append(target_id)
        self.reverse_adjacency[target_id].append(source_id)
```

### 2. Dependency Relationship Modeling

Model complex dependencies with different edge types:

```python
class EdgeType(Enum):
    DATA_DEPENDENCY = "data"      # Data flows from source to target
    CONTROL_DEPENDENCY = "control"  # Target waits for source completion
    CONDITIONAL_DEPENDENCY = "conditional"  # Target executes based on source result
    PARALLEL_DEPENDENCY = "parallel"  # Steps can execute in parallel

class EdgeMetadata:
    def __init__(self, source_id: str, target_id: str, edge_type: EdgeType):
        self.source_id = source_id
        self.target_id = target_id
        self.edge_type = edge_type
        self.data_mapping = {}  # source_output -> target_input mapping
        self.conditions = []    # Conditional execution rules
        self.weight = 1.0      # Edge weight for optimization

class StepNode:
    def __init__(self, step_id: str, step_spec: StepSpecification):
        self.step_id = step_id
        self.step_spec = step_spec
        self.node_type = step_spec.node_type
        self.execution_state = ExecutionState.PENDING
        self.dependencies_satisfied = False
        self.estimated_duration = None
        self.resource_requirements = {}
```

### 3. Execution Flow Control

Determine optimal execution order and enable parallelization:

```python
class ExecutionPlanner:
    def __init__(self, dag: PipelineDAG):
        self.dag = dag
    
    def get_topological_order(self) -> List[str]:
        """Get topologically sorted execution order"""
        in_degree = {node_id: 0 for node_id in self.dag.nodes}
        
        # Calculate in-degrees
        for source_id, targets in self.dag.adjacency_list.items():
            for target_id in targets:
                in_degree[target_id] += 1
        
        # Kahn's algorithm for topological sorting
        queue = deque([node_id for node_id, degree in in_degree.items() if degree == 0])
        result = []
        
        while queue:
            current = queue.popleft()
            result.append(current)
            
            for neighbor in self.dag.adjacency_list[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        if len(result) != len(self.dag.nodes):
            raise CycleDetectedError("DAG contains cycles")
        
        return result
    
    def get_parallel_execution_groups(self) -> List[List[str]]:
        """Group steps that can execute in parallel"""
        topo_order = self.get_topological_order()
        groups = []
        processed = set()
        
        for step_id in topo_order:
            if step_id in processed:
                continue
            
            # Find all steps that can execute in parallel with this step
            parallel_group = [step_id]
            processed.add(step_id)
            
            # Check remaining steps for parallel execution possibility
            for other_step_id in topo_order:
                if (other_step_id not in processed and 
                    self._can_execute_in_parallel(step_id, other_step_id)):
                    parallel_group.append(other_step_id)
                    processed.add(other_step_id)
            
            groups.append(parallel_group)
        
        return groups
```

### 4. Cycle Detection and Prevention

Ensure pipeline validity through comprehensive cycle detection:

```python
class CycleDetector:
    def __init__(self, dag: PipelineDAG):
        self.dag = dag
    
    def has_cycle(self) -> bool:
        """Detect if DAG contains cycles using DFS"""
        color = {node_id: 'WHITE' for node_id in self.dag.nodes}
        
        def dfs_visit(node_id):
            if color[node_id] == 'GRAY':
                return True  # Back edge found - cycle detected
            
            if color[node_id] == 'BLACK':
                return False  # Already processed
            
            color[node_id] = 'GRAY'
            
            for neighbor in self.dag.adjacency_list[node_id]:
                if dfs_visit(neighbor):
                    return True
            
            color[node_id] = 'BLACK'
            return False
        
        for node_id in self.dag.nodes:
            if color[node_id] == 'WHITE':
                if dfs_visit(node_id):
                    return True
        
        return False
    
    def find_cycles(self) -> List[List[str]]:
        """Find all cycles in the DAG"""
        cycles = []
        color = {node_id: 'WHITE' for node_id in self.dag.nodes}
        path = []
        
        def dfs_visit(node_id):
            if color[node_id] == 'GRAY':
                # Found cycle - extract it from path
                cycle_start = path.index(node_id)
                cycle = path[cycle_start:] + [node_id]
                cycles.append(cycle)
                return
            
            if color[node_id] == 'BLACK':
                return
            
            color[node_id] = 'GRAY'
            path.append(node_id)
            
            for neighbor in self.dag.adjacency_list[node_id]:
                dfs_visit(neighbor)
            
            path.pop()
            color[node_id] = 'BLACK'
        
        for node_id in self.dag.nodes:
            if color[node_id] == 'WHITE':
                dfs_visit(node_id)
        
        return cycles
```

### 5. Graph Analysis and Optimization

Enable advanced pipeline analysis and optimization:

```python
class DAGAnalyzer:
    def __init__(self, dag: PipelineDAG):
        self.dag = dag
    
    def find_critical_path(self) -> Tuple[List[str], float]:
        """Find the critical path (longest path) through the DAG"""
        topo_order = self._get_topological_order()
        distances = {node_id: 0 for node_id in self.dag.nodes}
        predecessors = {node_id: None for node_id in self.dag.nodes}
        
        # Calculate longest distances
        for node_id in topo_order:
            node = self.dag.nodes[node_id]
            current_duration = node.estimated_duration or 0
            
            for neighbor in self.dag.adjacency_list[node_id]:
                new_distance = distances[node_id] + current_duration
                if new_distance > distances[neighbor]:
                    distances[neighbor] = new_distance
                    predecessors[neighbor] = node_id
        
        # Find the node with maximum distance
        max_distance = max(distances.values())
        end_node = max(distances, key=distances.get)
        
        # Reconstruct critical path
        path = []
        current = end_node
        while current is not None:
            path.append(current)
            current = predecessors[current]
        
        return list(reversed(path)), max_distance
    
    def analyze_resource_utilization(self) -> Dict[str, Any]:
        """Analyze resource utilization across the pipeline"""
        parallel_groups = self._get_parallel_execution_groups()
        resource_usage = defaultdict(list)
        
        for group in parallel_groups:
            group_resources = defaultdict(int)
            for step_id in group:
                node = self.dag.nodes[step_id]
                for resource, amount in node.resource_requirements.items():
                    group_resources[resource] += amount
            
            for resource, total in group_resources.items():
                resource_usage[resource].append(total)
        
        return {
            "peak_usage": {resource: max(usage) for resource, usage in resource_usage.items()},
            "average_usage": {resource: sum(usage) / len(usage) for resource, usage in resource_usage.items()},
            "utilization_timeline": dict(resource_usage)
        }
    
    def suggest_optimizations(self) -> List[str]:
        """Suggest pipeline optimizations based on DAG analysis"""
        suggestions = []
        
        # Check for unnecessary sequential dependencies
        for source_id, targets in self.dag.adjacency_list.items():
            for target_id in targets:
                if self._can_be_parallelized(source_id, target_id):
                    suggestions.append(f"Consider parallelizing {source_id} and {target_id}")
        
        # Check for resource bottlenecks
        resource_analysis = self.analyze_resource_utilization()
        for resource, peak in resource_analysis["peak_usage"].items():
            if peak > self._get_resource_threshold(resource):
                suggestions.append(f"Resource bottleneck detected for {resource}: {peak}")
        
        # Check for long critical path
        critical_path, duration = self.find_critical_path()
        if duration > self._get_duration_threshold():
            suggestions.append(f"Critical path too long ({duration}): {' -> '.join(critical_path)}")
        
        return suggestions
```

## Integration with Other Components

### With Step Specifications

Pipeline DAG uses step specifications for node validation:

```python
class SpecificationAwareDAG(PipelineDAG):
    def add_node(self, step_id: str, step_spec: StepSpecification) -> StepNode:
        """Add node with specification validation"""
        node = super().add_node(step_id, step_spec)
        
        # Validate node type constraints
        if step_spec.node_type == NodeType.SOURCE:
            if self._has_incoming_edges(step_id):
                raise ValidationError(f"SOURCE node {step_id} cannot have dependencies")
        
        elif step_spec.node_type == NodeType.SINK:
            if self._has_outgoing_edges(step_id):
                raise ValidationError(f"SINK node {step_id} cannot have outputs")
        
        return node
    
    def add_edge_from_specs(self, source_id: str, target_id: str, 
                           output_name: str, dependency_name: str) -> None:
        """Add edge based on step specifications"""
        source_spec = self.nodes[source_id].step_spec
        target_spec = self.nodes[target_id].step_spec
        
        # Validate output exists
        if output_name not in source_spec.outputs:
            raise ValidationError(f"Output {output_name} not found in {source_id}")
        
        # Validate dependency exists
        if dependency_name not in target_spec.dependencies:
            raise ValidationError(f"Dependency {dependency_name} not found in {target_id}")
        
        # Check compatibility
        output_spec = source_spec.outputs[output_name]
        dep_spec = target_spec.dependencies[dependency_name]
        
        if not self._is_compatible(output_spec, dep_spec):
            raise ValidationError(f"Incompatible connection: {output_name} -> {dependency_name}")
        
        self.add_edge(source_id, target_id, EdgeType.DATA_DEPENDENCY)
```

### With Smart Proxies

Smart Proxies use DAG for connection validation:

```python
class DAGAwareSmartProxy:
    def __init__(self, step_id: str, config, dag: PipelineDAG):
        self.step_id = step_id
        self.config = config
        self.dag = dag
    
    def connect_from(self, source_proxy: 'DAGAwareSmartProxy', output_name: str = None):
        """Connect with DAG validation"""
        
        # Check if connection would create cycle
        if self.dag._would_create_cycle(source_proxy.step_id, self.step_id):
            raise CycleDetectedError(
                f"Connection from {source_proxy.step_id} to {self.step_id} would create cycle"
            )
        
        # Add edge to DAG
        self.dag.add_edge(source_proxy.step_id, self.step_id, EdgeType.DATA_DEPENDENCY)
        
        return self
```

### With Pipeline Execution

DAG provides execution planning for pipeline runners:

```python
class DAGBasedPipelineExecutor:
    def __init__(self, dag: PipelineDAG):
        self.dag = dag
        self.execution_planner = ExecutionPlanner(dag)
    
    def execute_pipeline(self) -> PipelineExecutionResult:
        """Execute pipeline based on DAG topology"""
        
        # Get execution plan
        parallel_groups = self.execution_planner.get_parallel_execution_groups()
        
        results = {}
        for group in parallel_groups:
            # Execute steps in parallel within each group
            group_results = self._execute_parallel_group(group)
            results.update(group_results)
            
            # Update DAG state
            for step_id in group:
                self.dag.nodes[step_id].execution_state = ExecutionState.COMPLETED
        
        return PipelineExecutionResult(results)
    
    def _execute_parallel_group(self, step_ids: List[str]) -> Dict[str, Any]:
        """Execute a group of steps in parallel"""
        with ThreadPoolExecutor(max_workers=len(step_ids)) as executor:
            futures = {
                executor.submit(self._execute_step, step_id): step_id 
                for step_id in step_ids
            }
            
            results = {}
            for future in as_completed(futures):
                step_id = futures[future]
                try:
                    results[step_id] = future.result()
                except Exception as e:
                    self._handle_step_failure(step_id, e)
            
            return results
```

## Strategic Value

Pipeline DAG provides:

1. **Structural Integrity**: Mathematical foundation ensures pipeline validity
2. **Execution Optimization**: Enable parallel execution and resource optimization
3. **Dependency Management**: Clear modeling of complex step relationships
4. **Cycle Prevention**: Automatic detection and prevention of invalid pipelines
5. **Analysis Capabilities**: Enable advanced pipeline analysis and optimization
6. **Scalability**: Efficient algorithms for large, complex pipelines

## Example Usage

```python
# Create and build a pipeline DAG
dag = PipelineDAG()

# Add nodes
data_node = dag.add_node("data_loading", DATA_LOADING_SPEC)
preprocess_node = dag.add_node("preprocessing", PREPROCESSING_SPEC)
training_node = dag.add_node("xgboost_training", XGBOOST_TRAINING_SPEC)
packaging_node = dag.add_node("model_packaging", PACKAGING_SPEC)
registration_node = dag.add_node("model_registration", REGISTRATION_SPEC)

# Add edges (dependencies)
dag.add_edge("data_loading", "preprocessing", EdgeType.DATA_DEPENDENCY)
dag.add_edge("preprocessing", "xgboost_training", EdgeType.DATA_DEPENDENCY)
dag.add_edge("xgboost_training", "model_packaging", EdgeType.DATA_DEPENDENCY)
dag.add_edge("model_packaging", "model_registration", EdgeType.DATA_DEPENDENCY)

# Validate DAG
if dag.has_cycle():
    raise ValueError("Pipeline contains cycles")

# Analyze execution plan
planner = ExecutionPlanner(dag)
execution_order = planner.get_topological_order()
parallel_groups = planner.get_parallel_execution_groups()

print(f"Execution order: {execution_order}")
print(f"Parallel groups: {parallel_groups}")

# Analyze pipeline
analyzer = DAGAnalyzer(dag)
critical_path, duration = analyzer.find_critical_path()
optimizations = analyzer.suggest_optimizations()

print(f"Critical path: {' -> '.join(critical_path)} ({duration} minutes)")
print(f"Suggested optimizations: {optimizations}")
```

Pipeline DAG serves as the **mathematical and computational backbone** of the pipeline architecture, providing the structural foundation that enables all higher-level abstractions to work correctly and efficiently while ensuring pipeline validity and enabling advanced optimization capabilities.
