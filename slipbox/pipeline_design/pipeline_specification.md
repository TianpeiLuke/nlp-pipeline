# Pipeline Specification

## What is the Purpose of Pipeline Specification?

Pipeline Specifications serve as **declarative blueprints** that define the complete structure, configuration, and quality requirements for ML pipelines. They represent the evolution from imperative pipeline construction to declarative, specification-driven pipeline definition.

## Core Purpose

Pipeline Specifications provide the **declarative blueprint layer** that:

1. **Declarative Pipeline Definition** - Define complete pipeline structure in a single specification
2. **Configuration Integration** - Leverage existing config classes without duplication
3. **Type Safety and Validation** - Ensure pipeline consistency through specification validation
4. **Quality Contract Integration** - Embed quality requirements directly in pipeline definition
5. **Execution Environment Configuration** - Specify runtime requirements and constraints

## Key Features

### 1. Declarative Pipeline Definition

Pipeline Specifications enable complete pipeline definition in a single, declarative format:

```python
@dataclass
class PipelineSpec:
    """
    Declarative specification for ML pipelines.
    
    Defines the complete pipeline structure using existing config classes
    and step specifications for type safety and dependency resolution.
    """
    # Pipeline metadata
    name: str
    description: str
    version: str = "1.0"
    tags: Dict[str, str] = field(default_factory=dict)
    
    # Step configuration using existing config classes
    step_configs: Dict[str, BasePipelineConfig]  # step_name -> config instance
    
    # Step specifications for type safety and dependency resolution
    step_specifications: Dict[str, StepSpecification]  # step_name -> specification
    
    # Optional quality contracts
    step_contracts: Optional[Dict[str, StepContract]] = None
    
    # Pipeline-level configuration
    pipeline_parameters: List[ParameterString] = field(default_factory=list)
    global_config: Dict[str, Any] = field(default_factory=dict)
    
    # Execution configuration
    execution_config: Optional[PipelineExecutionConfig] = None
```

### 2. Configuration Integration with Existing Config Classes

Seamlessly integrate with existing [Config](config.md) classes without duplication:

```python
# Define pipeline using existing config classes - no changes needed!
fraud_pipeline_spec = PipelineSpec(
    name="fraud_detection_v2",
    description="End-to-end fraud detection pipeline",
    
    # Reuse existing config classes directly
    step_configs={
        "data_loading": CradleDataLoadingStepConfig(
            s3_bucket="fraud-data-bucket",
            data_source="transactions",
            output_names={"data": "DATA", "metadata": "METADATA"}
        ),
        "preprocessing": TabularPreprocessingStepConfig(
            job_type="training",
            instance_type="ml.m5.xlarge",
            input_names={"data": "DATA"},
            output_names={"processed_data": "ProcessedTabularData"}
        ),
        "training": XGBoostTrainingStepConfig(
            instance_type="ml.m5.xlarge",
            max_depth=6,
            n_estimators=100,
            input_names={"training_data": "ProcessedTabularData"},
            output_names={"model": "ModelArtifacts"}
        )
    }
)
```

### 3. Type Safety Through Step Specifications

Leverage [Step Specifications](step_specification.md) for compile-time validation and dependency resolution:

```python
# Reference step specifications for type safety
step_specifications={
    "data_loading": CRADLE_DATA_LOADING_SPEC,
    "preprocessing": TABULAR_PREPROCESSING_SPEC,
    "training": XGBOOST_TRAINING_SPEC
}

# The specification validates:
# - All step configs have corresponding specifications
# - All specifications have corresponding configs
# - Step dependencies are properly typed
# - Input/output port compatibility
```

### 4. Quality Contract Integration

Embed [Step Contracts](step_contract.md) directly in pipeline specifications:

```python
# Optional contracts for quality gates
step_contracts={
    "training": XGBoostModelContract(
        performance_guarantees={
            "min_auc": 0.85,
            "max_training_time": "2 hours"
        },
        output_quality_checks=[
            ModelValidation(),
            PerformanceThreshold(min_auc=0.85)
        ]
    ),
    "preprocessing": DataQualityContract(
        input_quality_checks=[
            NoMissingValues(),
            ValidFeatureTypes()
        ]
    )
}
```

### 5. Execution Environment Configuration

Specify runtime requirements and execution constraints:

```python
@dataclass
class PipelineExecutionConfig:
    """Configuration for pipeline execution environment."""
    sagemaker_session: Optional[PipelineSession] = None
    role: Optional[str] = None
    default_bucket: Optional[str] = None
    kms_key: Optional[str] = None
    network_config: Optional[Dict[str, Any]] = None
    retry_config: Optional[Dict[str, Any]] = None
    
    # Resource constraints
    max_parallel_steps: Optional[int] = None
    timeout_minutes: Optional[int] = None
    
    # Monitoring and logging
    enable_monitoring: bool = True
    log_level: str = "INFO"
    metrics_config: Optional[Dict[str, Any]] = None

# Usage in pipeline specification
execution_config = PipelineExecutionConfig(
    role="arn:aws:iam::123456789012:role/SageMakerExecutionRole",
    default_bucket="ml-pipeline-artifacts",
    kms_key="arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012",
    max_parallel_steps=3,
    timeout_minutes=240,
    enable_monitoring=True
)
```

## Specification Validation

### Built-in Validation Rules

Pipeline Specifications include comprehensive validation:

```python
def validate(self) -> List[str]:
    """Validate pipeline specification consistency."""
    errors = []
    
    # Check that all step configs have corresponding specifications
    for step_name in self.step_configs:
        if step_name not in self.step_specifications:
            errors.append(f"Missing specification for step: {step_name}")
    
    # Check that all specifications have corresponding configs
    for step_name in self.step_specifications:
        if step_name not in self.step_configs:
            errors.append(f"Missing config for step: {step_name}")
    
    # Validate individual step specifications
    for step_name, spec in self.step_specifications.items():
        spec_errors = spec.validate()
        errors.extend([f"Step '{step_name}': {error}" for error in spec_errors])
    
    # Validate step contracts if present
    if self.step_contracts:
        for step_name, contract in self.step_contracts.items():
            if step_name not in self.step_configs:
                errors.append(f"Contract specified for non-existent step: {step_name}")
    
    return errors
```

### Cross-Component Validation

Validate consistency across different architectural components:

```python
class PipelineSpecValidator:
    """Comprehensive validator for pipeline specifications."""
    
    def validate_specification(self, spec: PipelineSpec) -> ValidationResult:
        """Perform comprehensive validation."""
        errors = []
        warnings = []
        
        # Basic specification validation
        basic_errors = spec.validate()
        errors.extend(basic_errors)
        
        # Cross-component validation
        config_spec_errors = self._validate_config_specification_consistency(spec)
        errors.extend(config_spec_errors)
        
        # Contract compatibility validation
        contract_errors = self._validate_contract_compatibility(spec)
        errors.extend(contract_errors)
        
        # Resource requirement validation
        resource_warnings = self._validate_resource_requirements(spec)
        warnings.extend(resource_warnings)
        
        return ValidationResult(errors=errors, warnings=warnings)
    
    def _validate_config_specification_consistency(self, spec: PipelineSpec) -> List[str]:
        """Validate that configs match their specifications."""
        errors = []
        
        for step_name, config in spec.step_configs.items():
            if step_name not in spec.step_specifications:
                continue
                
            specification = spec.step_specifications[step_name]
            
            # Validate input/output names match specification ports
            if hasattr(config, 'input_names') and config.input_names:
                for input_name in config.input_names:
                    if input_name not in specification.dependencies:
                        errors.append(f"Step '{step_name}' config references unknown input: {input_name}")
            
            if hasattr(config, 'output_names') and config.output_names:
                for output_name in config.output_names:
                    if output_name not in specification.outputs:
                        errors.append(f"Step '{step_name}' config references unknown output: {output_name}")
        
        return errors
```

## Integration with Other Components

### With Enhanced DAG

Pipeline Specifications work seamlessly with [Enhanced DAG](pipeline_dag.md) for dependency resolution:

```python
class PipelineSpecificationDAGIntegration:
    """Integration between Pipeline Specifications and Enhanced DAG."""
    
    def create_enhanced_dag_from_spec(self, spec: PipelineSpec) -> EnhancedPipelineDAG:
        """Create Enhanced DAG from pipeline specification."""
        enhanced_dag = EnhancedPipelineDAG()
        
        # Register all step specifications
        for step_name, specification in spec.step_specifications.items():
            enhanced_dag.register_step_specification(step_name, specification)
        
        # Auto-resolve dependencies
        enhanced_dag.auto_resolve_dependencies(confidence_threshold=0.7)
        
        return enhanced_dag
```

### With Smart Proxies

Pipeline Specifications provide the configuration for [Smart Proxies](smart_proxy.md):

```python
class SpecificationProxyIntegration:
    """Integration between specifications and smart proxies."""
    
    def create_proxies_from_spec(self, spec: PipelineSpec) -> Dict[str, SmartStepProxy]:
        """Create smart proxies from pipeline specification."""
        proxies = {}
        
        for step_name, config in spec.step_configs.items():
            specification = spec.step_specifications[step_name]
            
            proxy = SmartStepProxy(
                step_name=step_name,
                config=config,
                specification=specification,
                global_config=spec.global_config
            )
            
            proxies[step_name] = proxy
        
        return proxies
```

### With Step Builders

Pipeline Specifications coordinate with [Step Builders](step_builder.md) through configuration:

```python
class SpecificationBuilderIntegration:
    """Integration between specifications and step builders."""
    
    def get_builder_for_step(self, spec: PipelineSpec, step_name: str) -> StepBuilderBase:
        """Get appropriate step builder for a step in the specification."""
        config = spec.step_configs[step_name]
        specification = spec.step_specifications[step_name]
        
        # Determine builder class from specification
        builder_class = self._get_builder_class(specification.step_type)
        
        # Create builder with config
        builder = builder_class(config)
        
        return builder
```

## Template Patterns

### Common Pipeline Templates

Pipeline Specifications enable reusable template patterns:

```python
class PipelineTemplateLibrary:
    """Library of common pipeline specification templates."""
    
    @staticmethod
    def classification_pipeline_template(
        data_source: str,
        model_type: str = "xgboost",
        s3_bucket: str = None
    ) -> PipelineSpec:
        """Template for classification pipelines."""
        
        return PipelineSpec(
            name=f"{model_type}_classification_pipeline",
            description=f"Classification pipeline using {model_type}",
            
            step_configs={
                "data_loading": CradleDataLoadingStepConfig(
                    data_source=data_source,
                    s3_bucket=s3_bucket or "default-ml-bucket"
                ),
                "preprocessing": TabularPreprocessingStepConfig(
                    job_type="training",
                    preprocessing_type="classification"
                ),
                "training": XGBoostTrainingStepConfig(
                    objective="binary:logistic",
                    eval_metric="auc"
                ) if model_type == "xgboost" else PyTorchTrainingStepConfig(
                    model_type="classification"
                )
            },
            
            step_specifications={
                "data_loading": CRADLE_DATA_LOADING_SPEC,
                "preprocessing": TABULAR_PREPROCESSING_SPEC,
                "training": XGBOOST_TRAINING_SPEC if model_type == "xgboost" else PYTORCH_TRAINING_SPEC
            },
            
            step_contracts={
                "training": ClassificationModelContract(min_auc=0.7)
            }
        )
    
    @staticmethod
    def regression_pipeline_template(
        data_source: str,
        model_type: str = "xgboost"
    ) -> PipelineSpec:
        """Template for regression pipelines."""
        
        return PipelineSpec(
            name=f"{model_type}_regression_pipeline",
            description=f"Regression pipeline using {model_type}",
            
            step_configs={
                "data_loading": CradleDataLoadingStepConfig(
                    data_source=data_source
                ),
                "preprocessing": TabularPreprocessingStepConfig(
                    job_type="training",
                    preprocessing_type="regression"
                ),
                "training": XGBoostTrainingStepConfig(
                    objective="reg:squarederror",
                    eval_metric="rmse"
                )
            },
            
            step_specifications={
                "data_loading": CRADLE_DATA_LOADING_SPEC,
                "preprocessing": TABULAR_PREPROCESSING_SPEC,
                "training": XGBOOST_TRAINING_SPEC
            },
            
            step_contracts={
                "training": RegressionModelContract(max_rmse=0.1)
            }
        )
```

### Specification Composition

Enable composition of pipeline specifications:

```python
class PipelineSpecComposer:
    """Compose complex pipeline specifications from simpler components."""
    
    def compose_specifications(self, *specs: PipelineSpec) -> PipelineSpec:
        """Compose multiple specifications into a single pipeline."""
        
        # Merge step configs
        merged_configs = {}
        merged_specifications = {}
        merged_contracts = {}
        
        for spec in specs:
            merged_configs.update(spec.step_configs)
            merged_specifications.update(spec.step_specifications)
            
            if spec.step_contracts:
                merged_contracts.update(spec.step_contracts)
        
        # Create composed specification
        return PipelineSpec(
            name=f"composed_pipeline_{len(specs)}_components",
            description="Composed pipeline from multiple specifications",
            step_configs=merged_configs,
            step_specifications=merged_specifications,
            step_contracts=merged_contracts if merged_contracts else None
        )
    
    def extend_specification(self, base_spec: PipelineSpec, 
                           additional_steps: Dict[str, Tuple[BasePipelineConfig, StepSpecification]]) -> PipelineSpec:
        """Extend a base specification with additional steps."""
        
        extended_configs = {**base_spec.step_configs}
        extended_specifications = {**base_spec.step_specifications}
        
        for step_name, (config, specification) in additional_steps.items():
            extended_configs[step_name] = config
            extended_specifications[step_name] = specification
        
        return PipelineSpec(
            name=f"{base_spec.name}_extended",
            description=f"Extended version of {base_spec.name}",
            version=base_spec.version,
            step_configs=extended_configs,
            step_specifications=extended_specifications,
            step_contracts=base_spec.step_contracts,
            global_config=base_spec.global_config,
            execution_config=base_spec.execution_config
        )
```

## Strategic Value

Pipeline Specifications provide:

1. **Declarative Definition** - Complete pipeline structure in single specification
2. **Configuration Reuse** - Leverage existing config classes without duplication
3. **Type Safety** - Compile-time validation through step specifications
4. **Quality Integration** - Built-in contract validation and quality gates
5. **Template Reusability** - Enable common pipeline patterns and composition
6. **Execution Control** - Comprehensive runtime configuration management
7. **Validation Completeness** - Cross-component consistency checking

## Example Usage

```python
# Define complete pipeline specification
fraud_detection_spec = PipelineSpec(
    name="fraud_detection_production_v2",
    description="Production fraud detection pipeline with quality gates",
    version="2.0",
    tags={"environment": "production", "model_type": "xgboost"},
    
    # Step configuration using existing config classes
    step_configs={
        "data_loading": CradleDataLoadingStepConfig(
            s3_bucket="fraud-data-prod",
            data_source="transactions",
            validation_enabled=True
        ),
        "feature_engineering": TabularPreprocessingStepConfig(
            job_type="training",
            instance_type="ml.m5.2xlarge",
            feature_selection_enabled=True
        ),
        "model_training": XGBoostTrainingStepConfig(
            instance_type="ml.m5.4xlarge",
            max_depth=8,
            n_estimators=200,
            early_stopping_rounds=10
        ),
        "model_evaluation": ModelEvaluationStepConfig(
            evaluation_metrics=["auc", "precision", "recall"],
            threshold_optimization=True
        )
    },
    
    # Type safety through step specifications
    step_specifications={
        "data_loading": CRADLE_DATA_LOADING_SPEC,
        "feature_engineering": TABULAR_PREPROCESSING_SPEC,
        "model_training": XGBOOST_TRAINING_SPEC,
        "model_evaluation": MODEL_EVALUATION_SPEC
    },
    
    # Quality contracts for production readiness
    step_contracts={
        "data_loading": DataQualityContract(
            min_data_quality_score=0.95,
            required_columns=["transaction_id", "amount", "merchant_id"]
        ),
        "model_training": FraudModelContract(
            min_auc=0.88,
            max_false_positive_rate=0.05,
            max_training_time="4 hours"
        ),
        "model_evaluation": ModelValidationContract(
            min_validation_auc=0.85,
            required_fairness_metrics=["demographic_parity", "equalized_odds"]
        )
    },
    
    # Global configuration
    global_config={
        "pipeline_s3_loc": "s3://fraud-ml-prod/pipelines",
        "model_registry_name": "fraud-detection-models",
        "monitoring_enabled": True
    },
    
    # Execution configuration
    execution_config=PipelineExecutionConfig(
        role="arn:aws:iam::123456789012:role/SageMakerProductionRole",
        default_bucket="fraud-ml-prod",
        kms_key="arn:aws:kms:us-east-1:123456789012:key/prod-key",
        max_parallel_steps=2,
        timeout_minutes=300,
        enable_monitoring=True,
        log_level="INFO"
    )
)

# Validate specification before use
validation_errors = fraud_detection_spec.validate()
if validation_errors:
    raise ValueError(f"Invalid specification: {validation_errors}")

# Use with pipeline template builder
from pipeline_template_builder import ModernPipelineTemplateBuilder
builder = ModernPipelineTemplateBuilder(fraud_detection_spec)
pipeline = builder.build_pipeline()
```

Pipeline Specifications represent the **declarative foundation** of the modern pipeline architecture, enabling complete pipeline definition through composition of existing architectural components while maintaining type safety, quality assurance, and execution control.
