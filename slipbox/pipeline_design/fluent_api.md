# Fluent API

## What is the Purpose of Fluent API?

The Fluent API serves as a **natural language-like interface** for pipeline construction that transforms complex, imperative pipeline building into an intuitive, readable, and maintainable experience. It enables method chaining that mirrors the logical flow of ML workflows.

## Core Purpose

The Fluent API provides the **natural language interface layer** that:

1. **Natural Language-Like Pipeline Construction** - Enable pipeline construction that reads like natural language
2. **Method Chaining for Workflow Composition** - Support method chaining that mirrors ML pipeline flow
3. **Context-Aware Step Configuration** - Maintain context throughout pipeline construction
4. **Progressive Disclosure of Complexity** - Provide multiple abstraction levels from simple to advanced
5. **Type Safety and IntelliSense Support** - Provide compile-time validation and IDE support

## Key Features

### 1. Natural Language-Like Pipeline Construction

The Fluent API enables pipeline construction that reads like natural language:

```python
# Traditional Approach - Verbose, imperative, hard to read
data_step = DataLoadingStep()
data_step.set_config(data_config)
data_step.set_inputs(input_dict)

preprocess_step = PreprocessingStep()
preprocess_step.set_config(preprocess_config)
preprocess_step.set_inputs({
    "DATA": data_step.properties.ProcessingOutputConfig.Outputs["DATA"].S3Output.S3Uri,
    "METADATA": data_step.properties.ProcessingOutputConfig.Outputs["METADATA"].S3Output.S3Uri
})

pipeline = Pipeline()
pipeline.add_step(data_step)
pipeline.add_step(preprocess_step)

# Fluent API Approach - Reads like natural language
pipeline = (Pipeline("xgboost-training")
    .load_data(from_source="s3://bucket/data")
    .preprocess(with_config=preprocess_config)
    .train_xgboost(with_hyperparameters=xgb_params)
    .package_model()
    .register_in_model_registry())
```

### 2. Method Chaining for Workflow Composition

Enable method chaining that mirrors the logical flow of ML pipelines:

```python
# Each method returns the pipeline object, enabling chaining
result = (pipeline
    .with_data_source("s3://training-data/")
    .apply_preprocessing(StandardScaler(), OneHotEncoder())
    .split_data(train_ratio=0.8, validation_ratio=0.2)
    .train_model(XGBoostClassifier())
    .evaluate_performance(metrics=["accuracy", "f1", "auc"])
    .deploy_if_performance_threshold(min_accuracy=0.85)
    .notify_on_completion(email="team@company.com"))
```

### 3. Context-Aware Step Configuration

Maintain context throughout the pipeline construction for intelligent defaults:

```python
# Context flows through the chain
pipeline = (Pipeline("fraud-detection")
    .for_classification_task()  # Sets context for subsequent steps
    .load_tabular_data("s3://data/")  # Knows it's classification + tabular
    .apply_standard_preprocessing()  # Uses classification-appropriate preprocessing
    .train_with_hyperparameter_tuning()  # Uses classification metrics
    .deploy_with_auto_scaling())  # Configures for classification inference

class FluentPipeline:
    def __init__(self, name: str):
        self.name = name
        self.context = PipelineContext()
        self.steps = []
    
    def for_classification_task(self) -> 'FluentPipeline':
        """Set pipeline context for classification"""
        self.context.task_type = "classification"
        self.context.default_metrics = ["accuracy", "precision", "recall", "auc"]
        self.context.default_objective = "binary:logistic"
        return self
    
    def for_regression_task(self) -> 'FluentPipeline':
        """Set pipeline context for regression"""
        self.context.task_type = "regression"
        self.context.default_metrics = ["rmse", "mae", "r2"]
        self.context.default_objective = "reg:squarederror"
        return self
```

### 4. Progressive Disclosure of Complexity

Provide multiple levels of abstraction from simple to advanced:

```python
# Level 1 - Simple: One-liner for quick prototyping
pipeline = Pipeline("quick-model").auto_train_xgboost("s3://data/")

# Level 2 - Configured: Basic configuration
pipeline = (Pipeline("configured-model")
    .load_data("s3://data/")
    .preprocess_with_defaults()
    .train_xgboost(max_depth=6, n_estimators=100))

# Level 3 - Advanced: Full control with custom configurations
pipeline = (Pipeline("advanced-model")
    .load_data("s3://data/", validation_schema=schema)
    .preprocess(custom_transformers=[...])
    .train_xgboost(hyperparameter_tuning=True, custom_metrics=[...])
    .validate_with_holdout_set()
    .deploy_with_canary_strategy())

# Level 4 - Expert: Complete customization
pipeline = (Pipeline("expert-model")
    .load_data("s3://data/")
        .with_custom_processor(MyDataProcessor())
        .with_validation_rules([DataQualityCheck(), SchemaValidation()])
    .preprocess()
        .with_feature_engineering(FeatureEngineer())
        .with_data_balancing(SMOTEBalancer())
    .train_xgboost()
        .with_custom_estimator(CustomXGBoost())
        .with_distributed_training(instances=4)
        .with_early_stopping(patience=10))
```

### 5. Type Safety and IntelliSense Support

Provide compile-time validation and IDE support:

```python
class TypedFluentPipeline:
    def load_data(self, source: str) -> 'DataLoadedPipeline':
        """Load data - returns DataLoadedPipeline type"""
        return DataLoadedPipeline(self, source)
    
class DataLoadedPipeline:
    def preprocess(self, **kwargs) -> 'PreprocessedPipeline':
        """Preprocess data - returns PreprocessedPipeline type"""
        return PreprocessedPipeline(self, **kwargs)
    
class PreprocessedPipeline:
    def train_xgboost(self, **kwargs) -> 'TrainedPipeline':
        """Train XGBoost model - returns TrainedPipeline type"""
        return TrainedPipeline(self, **kwargs)
    
    def train_pytorch(self, **kwargs) -> 'TrainedPipeline':
        """Train PyTorch model - returns TrainedPipeline type"""
        return TrainedPipeline(self, **kwargs)

# Usage with full type safety
pipeline = (Pipeline("typed-pipeline")
    .load_data("s3://data/")  # Returns DataLoadedPipeline
    .preprocess()  # Returns PreprocessedPipeline  
    .train_xgboost())  # Returns TrainedPipeline

# IDE knows what methods are available at each stage
# Invalid operations are caught at development time
```

## Fluent API Patterns

### 1. Builder Pattern Integration

```python
class FluentStepBuilder:
    def __init__(self, step_type: str):
        self.step_type = step_type
        self.config = {}
        self.connections = []
    
    def with_config(self, **config) -> 'FluentStepBuilder':
        """Configure step with fluent interface"""
        self.config.update(config)
        return self
    
    def connect_to(self, target_step) -> 'FluentStepBuilder':
        """Connect to target step"""
        self.connections.append(target_step)
        return self
    
    def build(self):
        """Build the actual step"""
        return self._create_step(self.config, self.connections)

# Usage
step = (FluentStepBuilder("XGBoostTraining")
    .with_config(instance_type="ml.m5.xlarge", max_depth=6)
    .connect_to(preprocessing_step)
    .build())
```

### 2. Conditional Pipeline Construction

Support conditional logic and dynamic pipeline construction:

```python
class ConditionalFluentPipeline:
    def when(self, condition: bool) -> 'ConditionalFluentPipeline':
        """Conditional execution"""
        self._condition = condition
        return self
    
    def then(self, action: Callable) -> 'ConditionalFluentPipeline':
        """Execute action if condition is true"""
        if self._condition:
            action(self)
        return self
    
    def otherwise(self, action: Callable) -> 'ConditionalFluentPipeline':
        """Execute action if condition is false"""
        if not self._condition:
            action(self)
        return self

# Usage
pipeline = (Pipeline("dynamic")
    .load_data("s3://data/")
    .when(use_hyperparameter_tuning)
        .then(lambda p: p.with_hyperparameter_tuning(trials=50))
    .otherwise(lambda p: p.with_fixed_hyperparameters(params))
    .when(deploy_to_production)
        .then(lambda p: p.deploy_with_monitoring())
    .otherwise(lambda p: p.save_model_artifacts()))
```

### 3. Reusable Pipeline Templates

Enable creation of reusable pipeline templates:

```python
class PipelineTemplate:
    @staticmethod
    def classification_pipeline(data_source: str, model_type: str = "xgboost") -> FluentPipeline:
        """Reusable template for classification pipelines"""
        base_pipeline = (Pipeline("classification-template")
            .load_data(data_source)
            .validate_data_quality()
            .apply_classification_preprocessing())
        
        if model_type == "xgboost":
            return base_pipeline.train_xgboost().evaluate_classification_metrics()
        elif model_type == "neural_network":
            return base_pipeline.train_neural_network().evaluate_classification_metrics()
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    @staticmethod
    def regression_pipeline(data_source: str, model_type: str = "xgboost") -> FluentPipeline:
        """Reusable template for regression pipelines"""
        base_pipeline = (Pipeline("regression-template")
            .load_data(data_source)
            .validate_data_quality()
            .apply_regression_preprocessing())
        
        if model_type == "xgboost":
            return base_pipeline.train_xgboost().evaluate_regression_metrics()
        elif model_type == "linear":
            return base_pipeline.train_linear_model().evaluate_regression_metrics()

# Usage
fraud_pipeline = PipelineTemplate.classification_pipeline("s3://fraud-data/", "xgboost")
churn_pipeline = PipelineTemplate.classification_pipeline("s3://churn-data/", "neural_network")
sales_pipeline = PipelineTemplate.regression_pipeline("s3://sales-data/", "xgboost")
```

## Integration with Other Components

### With Smart Proxies

Fluent APIs use [Smart Proxies](smart_proxy.md) as the underlying implementation:

```python
class FluentPipeline:
    def __init__(self, name: str):
        self.name = name
        self.proxies = []
    
    def load_data(self, **kwargs) -> 'FluentPipeline':
        """Fluent data loading using Smart Proxy"""
        config = DataLoadingStepConfig(**kwargs)
        proxy = DataLoadingProxy(config)  # Smart Proxy
        self.proxies.append(proxy)
        return self
    
    def preprocess(self, **kwargs) -> 'FluentPipeline':
        """Fluent preprocessing using Smart Proxy"""
        config = PreprocessingStepConfig(**kwargs)
        proxy = PreprocessingProxy(config)  # Smart Proxy
        
        # Auto-connect using Smart Proxy intelligence
        if self.proxies:
            last_proxy = self.proxies[-1]
            proxy.connect_from(last_proxy)
        
        self.proxies.append(proxy)
        return self
```

### With Step Specifications

Fluent APIs leverage [specifications](step_specification.md) for validation and intelligent behavior:

```python
class SpecificationAwareFluentAPI:
    def add_step(self, step_type: str) -> 'FluentStepProxy':
        """Add step with specification-driven validation"""
        spec = global_registry.get_specification(step_type)
        if not spec:
            raise ValueError(f"Unknown step type: {step_type}")
        
        proxy = FluentStepProxy(step_type, spec)
        return proxy
    
class FluentStepProxy:
    def __init__(self, step_type: str, specification: StepSpecification):
        self.step_type = step_type
        self.specification = specification
    
    def connect_to(self, target_step: str) -> 'FluentStepProxy':
        """Connect with specification validation"""
        target_spec = global_registry.get_specification(target_step)
        
        # Validate compatibility using specifications
        if not self._is_compatible(self.specification, target_spec):
            raise ConnectionError(f"Cannot connect {self.step_type} to {target_step}")
        
        return self
```

### With Step Contracts

Fluent APIs enforce [step contracts](step_contract.md) during construction:

```python
class ContractEnforcingFluentAPI:
    def train_xgboost(self, **kwargs) -> 'FluentPipeline':
        """Train XGBoost with contract enforcement"""
        
        # Check preconditions from step contract
        contract = self._get_step_contract("XGBoostTraining")
        precondition_errors = contract.validate_preconditions(self.current_state)
        
        if precondition_errors:
            raise ContractViolationError(
                f"XGBoost training preconditions not met: {precondition_errors}"
            )
        
        return self._add_training_step("xgboost", **kwargs)
```

## Error Prevention and Validation

Fluent APIs catch errors early in the construction process:

```python
class ValidatingFluentPipeline:
    def train_xgboost(self, **kwargs) -> 'FluentPipeline':
        """Train XGBoost with validation"""
        
        # Check if preprocessing step exists
        if not self._has_preprocessing_step():
            raise PipelineValidationError(
                "XGBoost training requires preprocessed data. "
                "Add .preprocess() before .train_xgboost()"
            )
        
        # Validate hyperparameters
        if "max_depth" in kwargs and kwargs["max_depth"] < 1:
            raise ValidationError("max_depth must be positive")
        
        return self._add_training_step("xgboost", **kwargs)
    
    def deploy(self) -> 'FluentPipeline':
        """Deploy with validation"""
        
        # Check if model training step exists
        if not self._has_training_step():
            raise PipelineValidationError(
                "Cannot deploy without trained model. "
                "Add a training step before .deploy()"
            )
        
        return self._add_deployment_step()
```

## Strategic Value

The Fluent API provides:

1. **Improved Readability**: Pipeline construction reads like natural language
2. **Reduced Learning Curve**: Intuitive interface for new developers
3. **Error Prevention**: Type safety and validation prevent common mistakes
4. **Enhanced Productivity**: Method chaining and IntelliSense support
5. **Maintainability**: Clear, expressive code that's easy to understand and modify
6. **Flexibility**: Progressive disclosure allows simple to advanced usage

## Example Usage

```python
# Complete fluent pipeline construction
pipeline = (Pipeline("fraud-detection-v2")
    # Data loading with validation
    .load_data("s3://fraud-data/raw/")
        .with_schema_validation(FraudDataSchema)
        .with_quality_checks([NoMissingValues(), ValidDateRange()])
    
    # Feature engineering
    .engineer_features()
        .with_categorical_encoding(OneHotEncoder())
        .with_numerical_scaling(StandardScaler())
        .with_feature_selection(SelectKBest(k=50))
    
    # Model training with hyperparameter tuning
    .train_xgboost()
        .with_hyperparameter_tuning(
            max_depth=range(3, 10),
            learning_rate=[0.1, 0.2, 0.3],
            n_estimators=[100, 200, 300]
        )
        .with_early_stopping(patience=10)
        .with_cross_validation(folds=5)
    
    # Model evaluation
    .evaluate_model()
        .with_metrics(["accuracy", "precision", "recall", "auc"])
        .with_threshold_optimization()
        .with_feature_importance_analysis()
    
    # Conditional deployment
    .when(lambda results: results.auc > 0.85)
        .then(lambda p: p.deploy_to_production()
            .with_auto_scaling(min_instances=2, max_instances=10)
            .with_monitoring(CloudWatchMetrics())
            .with_a_b_testing(traffic_split=0.1))
    .otherwise(lambda p: p.save_model_artifacts()
        .with_notification("Model performance below threshold"))
    
    # Execution
    .execute())
```

The Fluent API represents the **user-facing culmination** of the specification-driven architecture, transforming complex pipeline construction into an intuitive, natural language-like experience while maintaining all the power and safety of the underlying system.
